{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extending the Gaussian Naive Bayes code so that it handles missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My GaussianNB\n",
    "Reimplementation of a Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGaussianNB(BaseEstimator, ClassifierMixin):          \n",
    "    \n",
    "    missing_columns=[]\n",
    "    def fit(self, Xt, yt):\n",
    "        self.var_smoothing = 1e-9   # zero variance will cause division by zero errors.\n",
    "        self.Xt = Xt\n",
    "        self.yt = yt\n",
    "        self.n_feat = Xt.shape[1]\n",
    "        self.mus = {}\n",
    "        self.sig_sqs = {}\n",
    "        self.priors = {}\n",
    "\n",
    "        \n",
    "        c_dict = Counter(self.yt)\n",
    "        \n",
    "        for c in c_dict.keys():\n",
    "            self.mus[c] = np.zeros(self.n_feat) # where the means will be stored\n",
    "            self.sig_sqs[c] = np.zeros(self.n_feat) # where the variances will be stored\n",
    "            self.priors[c] = c_dict[c]/Xt.shape[0]\n",
    "            \n",
    "            mask = self.yt == c\n",
    "            X_tr_c = self.Xt[mask, :] # the rows for this class label\n",
    "            \n",
    "            for f in range(self.n_feat):\n",
    "                self.mus[c][f] = np.mean(X_tr_c[:,f])\n",
    "                self.sig_sqs[c][f] = np.var(X_tr_c[:,f] + self.var_smoothing)  #var              \n",
    "        #print(self.mus)\n",
    "        #print(self.sig_sqs)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Fit function for handling missing values univariate\n",
    "    def fit_missing_uni(self, Xt, yt,limit): # This function takes 3 arguments Trainining data, Training Features and limit of missing values\n",
    "        Xt=pd.DataFrame(Xt) # convert into data frame\n",
    "        percent_missing = list(Xt.isnull().sum() * 100 / len(Xt)) # check the percentage of missing values\n",
    "        columns_dropped=[] # initializing the list of columns need to be dropped\n",
    "        \n",
    "        for i in range(len(percent_missing)): \n",
    "            if percent_missing[i] >= limit: # check if the missing values percentage is greater than defined limit\n",
    "                columns_dropped.append(Xt.columns[i]) \n",
    "        \n",
    "        self.missing_columns= columns_dropped       \n",
    "        Xt=Xt.drop(columns_dropped,axis=1) # dropped the columnns having missing values greater than defined limits\n",
    "        \n",
    "        \n",
    "        if len(Xt.columns) ==0: # if all columns dropped just return and print the message\n",
    "            return\n",
    "        else:\n",
    "            for i in Xt.columns:  \n",
    "                Xt[i]=Xt[i].fillna(Xt[i].median()) # impute the missing values of features having less missing values than threshold\n",
    "\n",
    "        Xt=np.array(Xt)\n",
    "        \n",
    "        self.var_smoothing = 1e-9   # zero variance will cause division by zero errors.\n",
    "        self.Xt = Xt\n",
    "        self.yt = yt\n",
    "        self.n_feat = Xt.shape[1]\n",
    "        self.mus = {}\n",
    "        self.sig_sqs = {}\n",
    "        self.priors = {}\n",
    "\n",
    "        c_dict = Counter(self.yt)\n",
    "        \n",
    "        for c in c_dict.keys():\n",
    "            self.mus[c] = np.zeros(self.n_feat) # where the means will be stored\n",
    "            self.sig_sqs[c] = np.zeros(self.n_feat) # where the variances will be stored\n",
    "            self.priors[c] = c_dict[c]/Xt.shape[0]\n",
    "            \n",
    "            mask = self.yt == c\n",
    "            X_tr_c = self.Xt[mask, :] # the rows for this class label\n",
    "            \n",
    "            for f in range(self.n_feat):\n",
    "                self.mus[c][f] = np.mean(X_tr_c[:,f])\n",
    "                self.sig_sqs[c][f] = np.var(X_tr_c[:,f] + self.var_smoothing)  #var              \n",
    "        #print(self.mus)\n",
    "        #print(self.sig_sqs)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Fit function for handling missing values multivariate\n",
    "    def fit_missing_mul(self, Xt, yt,limit): # This function takes 3 arguments Trainining data, Training Features and limit of missing values\n",
    "        Xt=pd.DataFrame(Xt) # convert into data frame\n",
    "        percent_missing = list(Xt.isnull().sum() * 100 / len(Xt)) # check the percentage of missing values\n",
    "        columns_dropped=[] # initializing the list of columns need to be dropped\n",
    "        \n",
    "        for i in range(len(percent_missing)): \n",
    "            if percent_missing[i] >= limit: # check if the missing values percentage is greater than defined limit\n",
    "                columns_dropped.append(Xt.columns[i]) \n",
    "        \n",
    "        self.missing_columns= columns_dropped       \n",
    "        Xt=Xt.drop(columns_dropped,axis=1) # dropped the columnns having missing values greater than defined limits\n",
    "        \n",
    "        \n",
    "        if len(Xt.columns) ==0: # if all columns dropped just return and print the message\n",
    "            return\n",
    "        else:\n",
    "            #for i in Xt.columns:  \n",
    "            Xt = Xt.fillna( Xt.median()) # impute the missing values of features having less missing values than threshold\n",
    "\n",
    "        Xt=np.array(Xt)\n",
    "        \n",
    "        self.var_smoothing = 1e-9   # zero variance will cause division by zero errors.\n",
    "        self.Xt = Xt\n",
    "        self.yt = yt\n",
    "        self.n_feat = Xt.shape[1]\n",
    "        self.mus = {}\n",
    "        self.sig_sqs = {}\n",
    "        self.priors = {}\n",
    "\n",
    "        c_dict = Counter(self.yt)\n",
    "        \n",
    "        for c in c_dict.keys():\n",
    "            self.mus[c] = np.zeros(self.n_feat) # where the means will be stored\n",
    "            self.sig_sqs[c] = np.zeros(self.n_feat) # where the variances will be stored\n",
    "            self.priors[c] = c_dict[c]/Xt.shape[0]\n",
    "            \n",
    "            mask = self.yt == c\n",
    "            X_tr_c = self.Xt[mask, :] # the rows for this class label\n",
    "            \n",
    "            for f in range(self.n_feat):\n",
    "                self.mus[c][f] = np.mean(X_tr_c[:,f])\n",
    "                self.sig_sqs[c][f] = np.var(X_tr_c[:,f] + self.var_smoothing)  #var              \n",
    "        #print(self.mus)\n",
    "        #print(self.sig_sqs)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # The predictions for univariate imputation.\n",
    "    def predict_uni(self, Xtes):\n",
    "        Xtes=pd.DataFrame(Xtes)\n",
    "        Xtes=Xtes.drop(self.missing_columns,axis=1) # dropping the features that we dropped in training data\n",
    "        \n",
    "        for i in Xtes.columns:   # Handling missing values univariate\n",
    "                Xtes[i]=Xtes[i].fillna(Xtes[i].median()) \n",
    "          \n",
    "        Xtes=np.array(Xtes)\n",
    "        \n",
    "        self.Xtes = Xtes\n",
    "         \n",
    "        res_list = []\n",
    "        for sample in Xtes:\n",
    "            res_list.append(self.predict_single(sample))\n",
    "            \n",
    "        return np.array(res_list)\n",
    "    \n",
    "     # The predictions for multi-variate imputation.\n",
    "    def predict_mul(self, Xtes):\n",
    "        Xtes=pd.DataFrame(Xtes)\n",
    "        Xtes=Xtes.drop(self.missing_columns,axis=1) # dropping the features that we dropped in training data\n",
    "        \n",
    "        # Handling missing values univariate\n",
    "        Xtes=Xtes.fillna(Xtes.median()) \n",
    "        \n",
    "        Xtes=np.array(Xtes)\n",
    "        \n",
    "        self.Xtes = Xtes\n",
    "         \n",
    "        res_list = []\n",
    "        for sample in Xtes:\n",
    "            res_list.append(self.predict_single(sample))\n",
    "            \n",
    "        return np.array(res_list)\n",
    "    \n",
    "        # The predictions are the most common class in the training set.\n",
    "    def predict(self, Xtes):\n",
    "        #print(\"Predicting MGNB\")\n",
    "        self.Xtes = Xtes\n",
    "         \n",
    "        res_list = []\n",
    "        for sample in Xtes:\n",
    "            res_list.append(self.predict_single(sample))\n",
    "            \n",
    "        return np.array(res_list)\n",
    "    \n",
    "    \n",
    "    def predict_single(self, x_single):\n",
    "        probs = {}\n",
    "        for c in self.priors.keys():   # for each of the class labels\n",
    "            probs[c] = self.priors[c]\n",
    "            for i, f in enumerate(x_single):\n",
    "                t1 = 1/math.sqrt(2*math.pi*self.sig_sqs[c][i])\n",
    "                num = (f - self.mus[c][i])**2\n",
    "                den = 2*self.sig_sqs[c][i]\n",
    "                pxi_y = t1 * math.exp(-num/den)\n",
    "                probs[c] = probs[c] * pxi_y\n",
    "                #print(t1, num, den, pxi_y)\n",
    "                #print(probs)\n",
    "            #print(c, self.priors[c])\n",
    "        return max(probs, key=probs.get) # Return the key with the largest value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commenting on my design decisions\n",
    "#### How did I handle the missing values?\n",
    "\n",
    "4 functions were created to handle the missing values.\n",
    "<br>\n",
    "1) fit_missing_uni(data, labels, missing)\n",
    "<br>\n",
    "This function takes 3 arguments as its input which are; data, labels and the percentage of missing values.<br>\n",
    "This function calculates the missing percentage of the missing values and removes the features having more missing values than the defined threshold. <br>\n",
    "Once the features are removed this function imputes the missing values by univariate Median.<br>\n",
    "\n",
    "<br>\n",
    "2) fit_missing_mul(data, labels, missing)\n",
    "<br>\n",
    "This function takes 3 arguments as its inputs which are; data, labels and the percentage of missing values.<br>\n",
    "This function calculates the missing percentage of the missing values and removes the features having missing values more than the defined threshold. <br>\n",
    "Once the features are removed this function imputes the missing values by multivariate Median.<br>\n",
    "\n",
    "3) predict_uni(data)\n",
    "<br>\n",
    "This function takes 1 argument which is the test data. <br>\n",
    "In the next step it removes the features that were selected by the \"fit_missing_uni\" function, due to exceeding the missing values threshold.<br>\n",
    "Once removing the column this functions replaces the missing values by the univariate median to handle the test data properly.<br>\n",
    "\n",
    "4) predict_mul()\n",
    "<br>\n",
    "This function takes 1 argument which is the test data. <br>\n",
    "In the next step, it removes the features that are selected by the \"fit_missing_mul\" function, due to exceeding the missing values threshold.<br>\n",
    "Once removing the column this functions replaces the missing values by the multivariate median to handle the test data properly.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test the performance of my implementation against the scikit-learn GaussianNB using missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penguins Dataset 20% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the limited percentage of missing values here, I've enterend 50%\n",
    "LIMIT=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 5)\n"
     ]
    }
   ],
   "source": [
    "# LOADING FILE WITH 20% MISSING VALUES\n",
    "df = pd.read_csv('PenguinsMV0.2.csv', index_col = 0, na_values = '?')\n",
    "print(df.shape)\n",
    "\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "y=df[\"species\"]\n",
    "X=df.drop([\"species\"],axis=1)\n",
    "\n",
    "# Scaling of data\n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)\n",
    "\n",
    "# Splitting of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=1/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.0 My Naive bayes For Handling Missing Values Itself univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My guassian naive bayes\n",
    "\n",
    "nb=MyGaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGaussianNB()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "nb.fit_missing_uni(X_train,y_train,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=nb.predict_uni(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8982035928143712\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validtion K-fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 1.0\n",
      "Model accuracy at fold 2 = 0.9701492537313433\n",
      "Model accuracy at fold 3 = 0.9850746268656716\n",
      "Model accuracy at fold 4 = 0.9848484848484849\n",
      "Model accuracy at fold 5 = 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    nb.fit_missing_uni(X_tr,y_tr,LIMIT)\n",
    "    predi=nb.predict_uni(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 My Naive bayes For Handling Missing Values Itself Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My guassian naive bayes\n",
    "\n",
    "nb=MyGaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGaussianNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "nb.fit_missing_mul(X_train,y_train,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=nb.predict_mul(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8982035928143712\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 1.0\n",
      "Model accuracy at fold 2 = 0.9701492537313433\n",
      "Model accuracy at fold 3 = 0.9850746268656716\n",
      "Model accuracy at fold 4 = 0.9848484848484849\n",
      "Model accuracy at fold 5 = 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    nb.fit_missing_mul(X_tr,y_tr,LIMIT)\n",
    "    predi=nb.predict_mul(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Naive Bayes Sciket Learn handling missing values Explicitly Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 5)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset again and imputing the missing values because naive bayes did not handle missing values in sk learn\n",
    "df = pd.read_csv('PenguinsMV0.2.csv', index_col = 0, na_values = '?')\n",
    "print(df.shape)\n",
    "\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "\n",
    "y=df[\"species\"]\n",
    "X=df.drop([\"species\"],axis=1)\n",
    "\n",
    "for i in X.columns:\n",
    "    X[i]= X[i].fillna(X[i].median())\n",
    "\n",
    "    \n",
    "# Scaling the dataset   \n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9401197604790419\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 0.9253731343283582\n",
      "Model accuracy at fold 2 = 0.8805970149253731\n",
      "Model accuracy at fold 3 = 0.9850746268656716\n",
      "Model accuracy at fold 4 = 0.9696969696969697\n",
      "Model accuracy at fold 5 = 0.25757575757575757\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    gnb.fit(X_tr,y_tr)\n",
    "    predi=gnb.predict(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2.1.3 My Naive Bayes handling missing values Explicitly Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=MyGaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGaussianNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=nb.predict_uni(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9401197604790419\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 0.9253731343283582\n",
      "Model accuracy at fold 2 = 0.8805970149253731\n",
      "Model accuracy at fold 3 = 0.9850746268656716\n",
      "Model accuracy at fold 4 = 0.9696969696969697\n",
      "Model accuracy at fold 5 = 0.25757575757575757\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    nb.fit(X_tr,y_tr)\n",
    "    predi=nb.predict(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Naive Bayes Sciket Learn handling missing values Explicitly Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 5)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset again and imputing the missing values because naive bayes did not handle missing values in sk learn\n",
    "df = pd.read_csv('PenguinsMV0.2.csv', index_col = 0, na_values = '?')\n",
    "print(df.shape)\n",
    "\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "\n",
    "\n",
    "y=df[\"species\"]\n",
    "X=df.drop([\"species\"],axis=1)\n",
    "\n",
    "\n",
    "X= X.fillna(X.median())\n",
    "\n",
    "    \n",
    "# Scaling the dataset   \n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9401197604790419\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 0.9253731343283582\n",
      "Model accuracy at fold 2 = 0.8805970149253731\n",
      "Model accuracy at fold 3 = 0.9850746268656716\n",
      "Model accuracy at fold 4 = 0.9696969696969697\n",
      "Model accuracy at fold 5 = 0.25757575757575757\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    gnb.fit(X_tr,y_tr)\n",
    "    predi=gnb.predict(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2.1.5 My Naive Bayes handling missing values Explicitly Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=MyGaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGaussianNB()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=nb.predict_uni(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9401197604790419\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 0.9253731343283582\n",
      "Model accuracy at fold 2 = 0.8805970149253731\n",
      "Model accuracy at fold 3 = 0.9850746268656716\n",
      "Model accuracy at fold 4 = 0.9696969696969697\n",
      "Model accuracy at fold 5 = 0.25757575757575757\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    nb.fit(X_tr,y_tr)\n",
    "    predi=nb.predict(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penguins Dataset using 40% missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now carry out further testing but this time using the Penguins dataset with 40% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('PenguinsMV0.4.csv', index_col = 0, na_values = '?')\n",
    "print(df.shape)\n",
    "\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "y=df[\"species\"]\n",
    "X=df.drop([\"species\"],axis=1)\n",
    "\n",
    "# Scaling of data\n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)\n",
    "\n",
    "# Splitting of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=1/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2.2.0 My Naive bayes For Handling Missing Values Itself univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My guassian naive bayes\n",
    "\n",
    "nb=MyGaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGaussianNB()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "nb.fit_missing_uni(X_train,y_train,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=nb.predict_uni(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8263473053892215\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validtion K-fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 1.0\n",
      "Model accuracy at fold 2 = 0.9850746268656716\n",
      "Model accuracy at fold 3 = 0.8805970149253731\n",
      "Model accuracy at fold 4 = 0.9696969696969697\n",
      "Model accuracy at fold 5 = 0.3181818181818182\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    nb.fit_missing_uni(X_tr,y_tr,LIMIT)\n",
    "    predi=nb.predict_uni(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2.2.1 My Naive bayes For Handling Missing Values Itself Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGaussianNB()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "nb.fit_missing_mul(X_train,y_train,LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=nb.predict_mul(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8263473053892215\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validtion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 1.0\n",
      "Model accuracy at fold 2 = 0.9850746268656716\n",
      "Model accuracy at fold 3 = 0.8805970149253731\n",
      "Model accuracy at fold 4 = 0.9696969696969697\n",
      "Model accuracy at fold 5 = 0.3181818181818182\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    nb.fit_missing_mul(X_tr,y_tr,LIMIT)\n",
    "    predi=nb.predict_mul(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Naive Bayes Sciket Learn handling missing values Explicitly Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 5)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset again and imputing the missing values because naive bayes did not handle missing values in sk learn\n",
    "df = pd.read_csv('penguinsMV0.4.csv', index_col = 0, na_values = '?')\n",
    "print(df.shape)\n",
    "\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y=df[\"species\"]\n",
    "X=df.drop([\"species\"],axis=1)\n",
    "\n",
    "for i in X.columns:\n",
    "    X[i]= X[i].fillna(X[i].median())\n",
    "\n",
    "    \n",
    "    \n",
    "# Scaling the dataset   \n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8083832335329342\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 0.6865671641791045\n",
      "Model accuracy at fold 2 = 0.6268656716417911\n",
      "Model accuracy at fold 3 = 0.8507462686567164\n",
      "Model accuracy at fold 4 = 0.9545454545454546\n",
      "Model accuracy at fold 5 = 0.0\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    gnb.fit(X_tr,y_tr)\n",
    "    predi=gnb.predict(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 My Naive Bayes handling missing values Explicitly Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=MyGaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGaussianNB()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=nb.predict_uni(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8083832335329342\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 0.6865671641791045\n",
      "Model accuracy at fold 2 = 0.6268656716417911\n",
      "Model accuracy at fold 3 = 0.8507462686567164\n",
      "Model accuracy at fold 4 = 0.9545454545454546\n",
      "Model accuracy at fold 5 = 0.0\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    nb.fit(X_tr,y_tr)\n",
    "    predi=nb.predict(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Naive Bayes Sciket Learn handling missing values Explicitly Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 5)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset again and imputing the missing values because naive bayes did not handle missing values in sk learn\n",
    "df = pd.read_csv('penguinsMV0.4.csv', index_col = 0, na_values = '?')\n",
    "print(df.shape)\n",
    "\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "\n",
    "\n",
    "y=df[\"species\"]\n",
    "X=df.drop([\"species\"],axis=1)\n",
    "\n",
    "X= X.fillna(X.median())\n",
    "\n",
    "    \n",
    "# Scaling the dataset   \n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8083832335329342\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 0.6865671641791045\n",
      "Model accuracy at fold 2 = 0.6268656716417911\n",
      "Model accuracy at fold 3 = 0.8507462686567164\n",
      "Model accuracy at fold 4 = 0.9545454545454546\n",
      "Model accuracy at fold 5 = 0.0\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    gnb.fit(X_tr,y_tr)\n",
    "    predi=gnb.predict(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.5 My Naive Bayes handling missing values Explicitly Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=MyGaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGaussianNB()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predi=nb.predict_uni(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8083832335329342\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of model\n",
    "print(accuracy_score(predi,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy at fold 1 = 0.6865671641791045\n",
      "Model accuracy at fold 2 = 0.6268656716417911\n",
      "Model accuracy at fold 3 = 0.8507462686567164\n",
      "Model accuracy at fold 4 = 0.9545454545454546\n",
      "Model accuracy at fold 5 = 0.0\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_tr, X_ts = X[train_index], X[test_index]\n",
    "    y_tr, y_ts = y[train_index], y[test_index]\n",
    "    nb.fit(X_tr,y_tr)\n",
    "    predi=nb.predict(X_ts)\n",
    "    print(\"Model accuracy at fold \"+str(i)+\" =\", accuracy_score(predi,y_ts))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commenting on the results of my evaluation\n",
    "\n",
    "We got comparable results to scikit learn when we dealt with the missing values both implicitly and explicitly in the sklearn naive bayes.<br>\n",
    "There is a minute difference between the accuracies of both models because we handle missing values implicitly and impute the median values. Due to this impution, there is a minor difference between the median of the whole dataset X and and a subset of the data set Xtest. The difference between the medians of both datasets cause a small disparity in the data set resulting in a slight difference in accuracy (approximately 1% - 2%).<br>\n",
    "<br>\n",
    "In the case of the other models, we obtained the same accuracy as the well as cross validation. This is because the median of the data by which we impute the missing values are the same.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
